#! /usr/bin/env python3
# -*- coding: utf-8 -*-
# vim:fenc=utf-8
#
# Copyright Â© 2017 Jerry Jia <jerryjiahaha@gmail.com>
#
# Distributed under terms of the MIT license.

# NOTE >=python3.6 required

"""
Download captcha images
"""

from collections import UserDict
from random import randint
import copy
import re
import asyncio

# XXX maybe I can use https://github.com/hellysmile/fake-useragent ...
USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A",
        "Mozilla/5.0 (iPad; CPU OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5355d Safari/8536.25",
        "Mozilla/5.0 (PLAYSTATION 3; 3.55)",
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1 QIHU 360SE",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; 360SE)",
        "Mozilla/5.0 (PlayStation 4 1.000) AppleWebKit/536.26 (KHTML, like Gecko)",
        "Mozilla/5.0 (Playstation Vita 1.61) AppleWebKit/531.22.8 (KHTML, like Gecko) Silk/3.2",
        "Mozilla/5.0 (compatible; Konqueror/4.1; OpenBSD) KHTML/4.1.4 (like Gecko)",
        "Mozilla/5.0 (compatible; Konqueror/4.5; FreeBSD) KHTML/4.5.4 (like Gecko)",
        "Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; ko; rv:1.9.1b2) Gecko/20081201 Firefox/3.1b2",
        ]


class http_header(UserDict):
    required_headers = ('Host', )
    _pattern = re.compile(r"[^a-zA-Z0-9]")
    _pattern2 = re.compile(r"[^a-zA-Z]")
    @staticmethod
    def camel(key: str):
        """ convert abc-def to Abc-Def format """
        sep_str = key.split('-')
        dispatchers = [http_header.validate, http_header.upper_first,]
        def app(key):
            for f in dispatchers:
                # f1(f0(key))
                key = f(key)
            return key
        output = []
        for key in sep_str:
            output.append(app(key))
#        list(map(lambda k: list(map(lambda f: f(k), dispatchers)), sep_str))
        return '-'.join(output)

    @staticmethod
    def validate(key):
        if http_header._pattern.search(key) or http_header._pattern2.match(key):
            raise KeyError(f"Invalid key {key}")
        return key

    @staticmethod
    def upper_first(key: str):
        k0 = ord(key[0])
        if k0 not in range(ord('A'), ord('Z')):
            k0 = k0 - ord('a') + ord('A')
            key = key.replace(key[0], chr(k0), 1)
        return key

    def __setitem__(self, key: str, val: str):
        self.data[self.camel(key)] = val

    def __getitem__(self, key: str):
        return self.data[self.camel(copy.copy(key))]

    @staticmethod
    def parseFrom(content: str, header = None):
        header = http_header() if header is None else header
        datas = content.split("\r\n")
        for data in datas:
            try:
                key, _, val = data.partition(": ")
                header[key] = val.strip(' ')
            except:
                pass
        return header

    def dump(self, req = True):
        output = []
        if req:
            for h in self.required_headers:
                try:
                    v = self.data[h]
                    output.append(f"{h}: {v}")
                except:
                    pass
        for k, v in self.data.items():
            if k not in self.required_headers:
                output.append(f"{k}: {v}")
        return "\r\n".join(output)

class http_client:
    protos = {
            'http': 80,
            'https': 443,
            }

    @staticmethod
    def extract_url(url: str):
        for p in http_client.protos.keys():
            urlhead = f"{p}://"
            found = url.find(urlhead)
            if found != -1:
                return (url[found + len(urlhead):], p,)
        return (url, 'http',)

    @staticmethod
    async def get(url: str, output = None):
        # step1 parse url
        url, proto = http_client.extract_url(url)
        print(url, proto)
        host = url.split('/')[0]
        if ':' in host:
            port = host.rpartition(':')[-1]
            host = host.rpartition(':')[0]
        else:
            port = http_client.protos[proto]
        path = '/' if host == url else url[len(host):]

        # step2 construct query
        ua = USER_AGENTS[randint(0, len(USER_AGENTS) - 1)]
        headers = http_header()
        headers['host'] = host
        headers['user-agent'] = ua
        headers['Accept'] = '*/*'
        dump_header = headers.dump()
        to_send = f"GET {path} HTTP/1.1\r\n{dump_header}\r\n\r\n"
        print(to_send)
        print(host, port)
        ssl = True if proto == 'https' else False

        # step3 send request
        reader, writer = await asyncio.open_connection(host, port, ssl = ssl)
        writer.write(to_send.encode())

        # step4 read response
        header = http_header()

        # step4.1 get http response state
        res_state = await asyncio.wait_for(reader.readline(), 10) # timeout: 10s

        # step4.2 get http response header
        while True:
            got = await asyncio.wait_for(reader.readline(), 10)
            print("got:", got)
            if got == b'\r\n':
                break
            header = http_header.parseFrom(got.decode(), header)
        print(header.dump(), False)
        try:
            body_length = int(header['Content-Length'])
        except KeyError:
            return

        # step 4.3 get http body
        body = b''
        while len(body) < body_length:
            body += await asyncio.wait_for(reader.readexactly(1), 10)
        print(body)

        # step 5 process http response
        if output is None:
            return
        f = open(output, 'wb')
        # TODO use async write
        f.write(body)


if __name__ == '__main__':
    from argparse import ArgumentParser
    parser = ArgumentParser()
    parser.add_argument('url', help = "request url")
    parser.add_argument('-o', '--output', help = "output filename")
    args = parser.parse_args()

    loop = asyncio.get_event_loop()

    cli = http_client.get(args.url, args.output)

    loop.run_until_complete(cli)
